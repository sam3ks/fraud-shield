{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install faker\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from faker import Faker\n",
        "from geopy.distance import geodesic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0F5CPelzPNP",
        "outputId": "31a352fe-afd5-41ac-d77b-766b9171f873",
        "collapsed": true
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (36.1.1)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Faker\n",
        "fake = Faker()\n",
        "\n",
        "# Parameters\n",
        "NUM_USERS = 1000\n",
        "FRAUD_RATIO = 0.04  # 4% fraud\n",
        "\n",
        "# List of Bengaluru-specific regions\n",
        "BENGALURU_REGIONS = {\n",
        "    'Koramangala': (12.9288, 77.6228),'Jayanagar': (12.9333, 77.5833),'Whitefield': (12.9764, 77.7513),'Indiranagar': (12.9701, 77.6402),'Malleshwaram': (13.0034, 77.5723),\n",
        "    'Hebbal': (13.0312, 77.5924),'Hennur': (13.0245, 77.6247),'Sarjapur Road': (12.9121, 77.6774),'Bannerghatta Road': (12.8786, 77.5900),'Electronic City': (12.8543, 77.6780),\n",
        "    'Kalyan Nagar': (13.0272, 77.6463),'BTM Layout': (12.9341, 77.5910),'Vijayanagar': (12.9557, 77.5500),'Bellandur': (12.9336, 77.6543),'Kengeri': (12.9202, 77.4856),\n",
        "    'Yelahanka': (13.1008, 77.5963),'Rajajinagar': (12.9917, 77.5568),'Marathahalli': (12.9561, 77.7017),'HSR Layout': (12.9121, 77.6446),'Nagawara': (13.0452, 77.6226),\n",
        "    'Devanahalli': (13.2485, 77.7132),'Attibele': (12.7762, 77.7672),'Nelamangala': (13.0982, 77.3935),'Hoskote': (13.0707, 77.7850),'Anekal': (12.7110, 77.6956)\n",
        "}\n",
        "\n",
        "# Region Probability Distributions\n",
        "USER_REGION_CHOICES = {\n",
        "    \"Jayanagar\": 0.15, \"Malleshwaram\": 0.15, \"Rajajinagar\": 0.15,\n",
        "    \"Whitefield\": 0.07, \"Marathahalli\": 0.07, \"Electronic City\": 0.07,\n",
        "    \"Bellandur\": 0.06, \"HSR Layout\": 0.06, \"Sarjapur Road\": 0.05,\n",
        "    \"Kengeri\": 0.06, \"Yelahanka\": 0.05, \"Hebbal\": 0.05, \"Hennur\": 0.04, \"Kalyan Nagar\": 0.04,\n",
        "    \"Devanahalli\": 0.03, \"Attibele\": 0.03, \"Nelamangala\": 0.02, \"Hoskote\": 0.02, \"Anekal\": 0.02\n",
        "}\n",
        "ORDER_REGION_CHOICES = {\n",
        "    \"Whitefield\": 0.15, \"Marathahalli\": 0.15, \"Bellandur\": 0.15, \"Electronic City\": 0.14, \"Indiranagar\": 0.13,\n",
        "    \"HSR Layout\": 0.10, \"Koramangala\": 0.10, \"Sarjapur Road\": 0.10, \"BTM Layout\": 0.10, \"Kalyan Nagar\": 0.08,\n",
        "    \"Jayanagar\": 0.07, \"Malleshwaram\": 0.07, \"Rajajinagar\": 0.07, \"Hebbal\": 0.06, \"Yelahanka\": 0.05,\n",
        "    \"Devanahalli\": 0.02, \"Anekal\": 0.02, \"Attibele\": 0.02, \"Nelamangala\": 0.01\n",
        "}\n",
        "RECEIVER_REGION_CHOICES = {\n",
        "    \"Whitefield\": 0.18, \"Marathahalli\": 0.18, \"Electronic City\": 0.18, \"Bellandur\": 0.17, \"HSR Layout\": 0.16,\n",
        "    \"Koramangala\": 0.12, \"Indiranagar\": 0.12, \"BTM Layout\": 0.12, \"Sarjapur Road\": 0.12,\n",
        "    \"Jayanagar\": 0.08, \"Malleshwaram\": 0.08, \"Rajajinagar\": 0.08, \"Yelahanka\": 0.07, \"Hebbal\": 0.07,\n",
        "    \"Devanahalli\": 0.04, \"Anekal\": 0.03, \"Attibele\": 0.03, \"Nelamangala\": 0.02, \"Hoskote\": 0.02\n",
        "}\n",
        "\n",
        "BENGALURU_REGION_NAMES = list(BENGALURU_REGIONS.keys())\n",
        "\n",
        "\n",
        "\n",
        "# List of Indian first names for email generation\n",
        "INDIAN_FIRST_NAMES = ['Amit', 'Ramesh', 'Suresh', 'Rajesh', 'Priya', 'Kiran', 'Sunil', 'Anjali', 'Neha', 'Manish',\n",
        "                      'Meena', 'Pooja', 'Vikas', 'Ravi', 'Rahul', 'Sonia', 'Deepak', 'Shreya', 'Kavita']\n",
        "# List of email domain names for email generation\n",
        "INDIAN_EMAIL_DOMAINS = ['gmail.com', 'yahoo.co.in', 'rediffmail.com', 'outlook.co.in', 'indiatimes.com',\n",
        "                        'aol.in', 'airtelmail.com', 'bsnl.in', 'zoho.com']\n",
        "# Function to generate indian email\n",
        "def generate_indian_email():\n",
        "    first_name = np.random.choice(INDIAN_FIRST_NAMES)\n",
        "    domain = np.random.choice(INDIAN_EMAIL_DOMAINS)\n",
        "    return f\"{first_name.lower()}@{domain}\"\n",
        "\n",
        "\n",
        "\n",
        "# Function to generate unique Indian phone number\n",
        "def generate_unique_indian_phone_number(existing_numbers):\n",
        "    while True:\n",
        "        phone_number = f\"+91 {random.randint(7000000000, 9999999999)}\"  # Indian phone number format\n",
        "        if phone_number not in existing_numbers:\n",
        "            existing_numbers.add(phone_number)\n",
        "            return phone_number\n",
        "\n",
        "\n",
        "# Generate a pool of unique phone numbers\n",
        "unique_phone_numbers = set()\n",
        "for _ in range(NUM_USERS * 3):  # Assuming 3 phone numbers per user on average\n",
        "    generate_unique_indian_phone_number(unique_phone_numbers)\n",
        "# Convert the set of unique phone numbers to a list\n",
        "unique_phone_numbers = list(unique_phone_numbers)\n",
        "\n",
        "\n",
        "# Generate probability distribution for card networks using exponential decay\n",
        "CARD_NETWORKS = [\"Visa\", \"Mastercard\", \"American Express\", \"Rupay\"]\n",
        "decay_factor = 1.0  # Adjust to control skewness\n",
        "synthetic_distribution = np.exp(-decay_factor * np.arange(len(CARD_NETWORKS)))\n",
        "synthetic_distribution /= synthetic_distribution.sum()  # Normalize\n",
        "\n",
        "\n",
        "# Generate probability distribution for card types using exponential decay (higher weight to \"Debit\")\n",
        "CARD_TYPES = ['Debit', 'Credit', 'Prepaid']\n",
        "decay_factor = 0.8  # Adjust for skewness\n",
        "raw_counts = np.exp(-decay_factor * np.arange(len(CARD_TYPES)))\n",
        "raw_counts /= raw_counts.sum()  # Normalize to sum to 1\n",
        "damping_factor = 2.0  # Apply a damping factor to balance smaller categories, Ensures \"Prepaid\" isn't underrepresented\n",
        "adjusted_counts = raw_counts * (1 + damping_factor * (raw_counts < np.median(raw_counts)))\n",
        "card_probabilities = adjusted_counts / adjusted_counts.sum()# Normalize final probabilities\n",
        "\n",
        "\n",
        "# Generate user data\n",
        "users_data = {\n",
        "    \"User_ID\": np.arange(1000, NUM_USERS + 1000),  # Fix indexing\n",
        "    \"Sender_email\": [generate_indian_email() for _ in range(NUM_USERS)],\n",
        "    \"Card Number\": [fake.credit_card_number(card_type=\"visa16\") for _ in range(NUM_USERS)],\n",
        "    \"BIN Number\": [str(fake.credit_card_number(card_type=\"visa16\"))[:6] for _ in range(NUM_USERS)],  # FIXED\n",
        "    \"Card Network\": np.random.choice(CARD_NETWORKS, NUM_USERS, p=synthetic_distribution),\n",
        "    \"Card Tier\": np.random.choice(['Silver', 'Gold', 'Platinum', 'Black'], NUM_USERS),\n",
        "    \"Card Type\": np.random.choice(CARD_TYPES, size=NUM_USERS, p=card_probabilities),\n",
        "    \"User_Region\": np.random.choice(BENGALURU_REGION_NAMES, NUM_USERS),\n",
        "    \"transactions\":np.random.poisson(lam=10, size=NUM_USERS).clip(1, 50),\n",
        "}\n",
        "users_data[\"BIN Number\"] = [str(card)[:6] for card in users_data[\"Card Number\"]]\n",
        "\n",
        "\n",
        "#Assign cards\n",
        "users_df = pd.DataFrame(users_data)\n",
        "user_card_mapping = {}\n",
        "for user_id in users_df[\"User_ID\"]:\n",
        "    num_cards = np.random.randint(1, 4)\n",
        "    card_numbers = [fake.credit_card_number(card_type=\"visa16\") for _ in range(num_cards)]\n",
        "    user_card_mapping[user_id] = card_numbers\n",
        "\n",
        "\n",
        "# Step 1: Assign 1 to 3 unique phone numbers per user\n",
        "user_phone_mapping = {}  # Stores user -> list of phone numbers\n",
        "used_phone_numbers = set()\n",
        "for user_id in users_df[\"User_ID\"]:\n",
        "    num_phone_numbers = np.random.randint(1, 4)  # Each user gets 1 to 3 phone numbers\n",
        "    phone_numbers = [generate_unique_indian_phone_number(used_phone_numbers) for _ in range(num_phone_numbers)]\n",
        "    user_phone_mapping[user_id] = phone_numbers  # Store user's available phone numbers\n",
        "\n",
        "\n",
        "# Step 2: Assign phone numbers to each card (some may share numbers)\n",
        "card_phone_mapping = {}  # Stores card -> phone number\n",
        "for user_id, cards in user_card_mapping.items():\n",
        "    user_phones = user_phone_mapping[user_id]  # Get available phone numbers for this user\n",
        "    for card in cards:\n",
        "        card_phone_mapping[card] = random.choice(user_phones)  # Assign one of the user's numbers to this card\n",
        "\n",
        "\n",
        "# Ensure correct card network assignment\n",
        "def get_card_network(card_number, user_card_mapping, users_df):\n",
        "    for user_id, cards in user_card_mapping.items():\n",
        "        if card_number in cards:\n",
        "            return users_df.loc[users_df['User_ID'] == user_id, 'Card Network'].values[0]\n",
        "    return np.random.choice(CARD_NETWORKS, p=synthetic_distribution)  # Default if not found\n",
        "\n",
        "\n",
        "# Generate probability distribution for product categories using exponential decay (more weight to \"Wallet\")\n",
        "PRODUCT_CATEGORIES = ['Wallet', 'Consumable', 'Retail', 'Household', 'Services', 'Miscellaneous']\n",
        "decay_factor = 0.9  # Adjust to control skewness\n",
        "raw_counts = np.exp(-decay_factor * np.arange(len(PRODUCT_CATEGORIES)))\n",
        "raw_counts /= raw_counts.sum()  # Normalize to sum to 1\n",
        "damping_factor = 1.5  # Apply a damping factor to balance smaller categories (e.g., Household, Services, Miscellaneous) Ensures smaller categories are not underrepresented\n",
        "adjusted_counts = raw_counts * (1 + damping_factor * (raw_counts < np.median(raw_counts)))\n",
        "product_probabilities = adjusted_counts / adjusted_counts.sum()# Normalize final probabilities\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#Assigning regions\n",
        "user_region_mapping = {}\n",
        "def assign_regions(user_id):\n",
        "    \"\"\"\n",
        "    Assign User_Region, Order_Region, and Receiver_Region based on realistic scenarios.\n",
        "    Ensures that each user has a fixed User_Region.\n",
        "    \"\"\"\n",
        "    global user_region_mapping  # Ensure the mapping persists across function calls\n",
        "\n",
        "    # Assign User_Region permanently if not already assigned\n",
        "    if user_id not in user_region_mapping:\n",
        "        user_region_mapping[user_id] = random.choices(\n",
        "            list(USER_REGION_CHOICES.keys()), weights=USER_REGION_CHOICES.values(),k=1)[0]\n",
        "\n",
        "    user_region = user_region_mapping[user_id]\n",
        "\n",
        "    # Define probabilities for different scenarios\n",
        "    scenario = random.choices(\n",
        "        [\"all_same\", \"user_order_same\", \"order_receiver_same\", \"user_receiver_same\", \"all_different\"],\n",
        "        weights=[0.35, 0.20, 0.15, 0.15, 0.15],k=1)[0]\n",
        "\n",
        "    if scenario == \"all_same\":\n",
        "        order_region = user_region\n",
        "        receiver_region = user_region\n",
        "\n",
        "    elif scenario == \"user_order_same\":\n",
        "        order_region = user_region\n",
        "        receiver_region = random.choices(list(RECEIVER_REGION_CHOICES.keys()), weights=RECEIVER_REGION_CHOICES.values(), k=1)[0]\n",
        "        while receiver_region == user_region:\n",
        "            receiver_region = random.choices(list(RECEIVER_REGION_CHOICES.keys()), weights=RECEIVER_REGION_CHOICES.values(), k=1)[0]\n",
        "\n",
        "    elif scenario == \"order_receiver_same\":\n",
        "        order_region = random.choices(list(ORDER_REGION_CHOICES.keys()), weights=ORDER_REGION_CHOICES.values(), k=1)[0]\n",
        "        receiver_region = order_region\n",
        "        while receiver_region == user_region:\n",
        "            receiver_region = random.choices(list(RECEIVER_REGION_CHOICES.keys()), weights=RECEIVER_REGION_CHOICES.values(), k=1)[0]\n",
        "\n",
        "    elif scenario == \"user_receiver_same\":\n",
        "        receiver_region = user_region\n",
        "        order_region = random.choices(list(ORDER_REGION_CHOICES.keys()), weights=ORDER_REGION_CHOICES.values(), k=1)[0]\n",
        "        while order_region == user_region:\n",
        "            order_region = random.choices(list(ORDER_REGION_CHOICES.keys()), weights=ORDER_REGION_CHOICES.values(), k=1)[0]\n",
        "\n",
        "    elif scenario == \"all_different\":\n",
        "        order_region = random.choices(list(ORDER_REGION_CHOICES.keys()), weights=ORDER_REGION_CHOICES.values(), k=1)[0]\n",
        "        receiver_region = random.choices(list(RECEIVER_REGION_CHOICES.keys()), weights=RECEIVER_REGION_CHOICES.values(), k=1)[0]\n",
        "        while receiver_region == user_region or receiver_region == order_region:\n",
        "            receiver_region = random.choices(list(RECEIVER_REGION_CHOICES.keys()), weights=RECEIVER_REGION_CHOICES.values(), k=1)[0]\n",
        "\n",
        "    return user_region, order_region, receiver_region\n",
        "\n",
        "# Generate transaction data\n",
        "data = []\n",
        "transaction_id = 10000\n",
        "\n",
        "for _, user in users_df.iterrows():\n",
        "    user_id = user[\"User_ID\"]\n",
        "    user_region = user[\"User_Region\"]\n",
        "    num_user_transactions = user[\"transactions\"]\n",
        "\n",
        "    for _ in range(num_user_transactions):\n",
        "        # Pick a card randomly for this transaction\n",
        "        card_number = random.choice(user_card_mapping[user_id])\n",
        "        sender_email = user[\"Sender_email\"]  # Use the same email for all cards of the user\n",
        "\n",
        "        transaction = {\n",
        "            \"TransactionID\": transaction_id,\n",
        "            \"User_ID\": user_id,\n",
        "            \"User_Region\": user_region,\n",
        "            \"Sender_email\": sender_email,\n",
        "            \"Card Number\": card_number,\n",
        "            \"BIN Number\": user[\"BIN Number\"],\n",
        "            \"Card Network\": get_card_network(card_number, user_card_mapping, users_df),\n",
        "            \"Card Tier\": user[\"Card Tier\"],\n",
        "            \"Card Type\": user[\"Card Type\"],\n",
        "            \"Phone Numbers\": card_phone_mapping[card_number]\n",
        "        }\n",
        "        transaction_id += 1\n",
        "\n",
        "        user_region, order_region, receiver_region = assign_regions(transaction['User_ID'])\n",
        "        transaction[\"User_Region\"] = user_region\n",
        "        transaction[\"Order_Region\"] = order_region\n",
        "        transaction[\"Receiver_Region\"] = receiver_region\n",
        "\n",
        "        # Distance Calculation (Between Order_Region and Receiver_Region)\n",
        "        if transaction[\"Order_Region\"] == transaction[\"Receiver_Region\"]:\n",
        "          transaction[\"Distance\"] = np.round(np.random.uniform(0.1, 5), 2)  # Small within-region distance\n",
        "        else:\n",
        "          order_coords = BENGALURU_REGIONS[transaction[\"Order_Region\"]]\n",
        "          receiver_coords = BENGALURU_REGIONS[transaction[\"Receiver_Region\"]]\n",
        "          transaction[\"Distance\"] = np.round(geodesic(order_coords, receiver_coords).km, 2)  # Actual distance\n",
        "\n",
        "\n",
        "        # Assign fraud label\n",
        "        # transaction[\"isFraud\"] = np.random.choice([0, 1], p=[1 - FRAUD_RATIO, FRAUD_RATIO])\n",
        "        transaction[\"isFraud\"]=0\n",
        "\n",
        "        # Assign TransactionAmt with different variance for fraud vs legit\n",
        "        mu = np.log(135.03)  # Log mean of the dataset\n",
        "        fraud_sigma = np.log(1 + (500 / 135.03))  # Higher variance for fraud\n",
        "        legit_sigma = np.log(1 + (239.16 / 135.03))  # Original value for legit\n",
        "\n",
        "        transaction['TransactionAmt'] = np.random.lognormal(\n",
        "                mean=mu,\n",
        "                sigma=fraud_sigma if transaction[\"isFraud\"] else legit_sigma,\n",
        "                size=1)[0]\n",
        "        transaction['TransactionAmt'] = np.clip(transaction['TransactionAmt'], 0.251, 31937.391)# Clip values to match min and max range\n",
        "\n",
        "\n",
        "\n",
        "        transaction[\"ProductCD\"] = np.random.choice(PRODUCT_CATEGORIES, p=product_probabilities)\n",
        "\n",
        "        # Define merchants under relevant ProductCD categories\n",
        "        MERCHANT_MAPPING = {\n",
        "                            \"Wallet\": [\"Flipkart\", \"Amazon\",\"Google Play\", \"BigBasket\", \"Uber\", \"Zomato\", \"Swiggy Instamart\"],  # Wallets are payment methods, not merchants\n",
        "                            \"Consumable\": [\"BigBasket\", \"Blinkit\", \"DMart\", \"JioMart\", \"Swiggy Instamart\", \"Zepto\", \"Natureâ€™s Basket\", \"MilkBasket\"],\n",
        "                            \"Retail\": [\"Flipkart\", \"Amazon\", \"Reliance Digital\", \"Croma\", \"Tata Cliq\", \"Myntra\", \"Nykaa\", \"Ajio\", \"Meesho\", \"Snapdeal\"],\n",
        "                            \"Household\": [\"Pepperfry\", \"Urban Ladder\", \"IKEA\", \"Wakefit\", \"Home Centre\", \"Nilkamal\", \"Durian\", \"Godrej Interio\", \"Hometown\"],\n",
        "                            \"Services\": [\"Netflix\", \"Amazon Prime\", \"Hotstar\", \"Spotify\", \"Zee5\", \"JioSaavn\", \"Unacademy\", \"Byju's\", \"ALT Balaji\", \"Sony LIV\", \"Audible\", \"Coursera\", \"Udemy\", \"Skillshare\"],\n",
        "                            \"Miscellaneous\": [\"Dream11\", \"RummyCircle\", \"PokerBaazi\", \"MPL\", \"Decathlon\", \"FirstCry\", \"Tata 1mg\", \"1x BET\", \"Betway\", \"Lottoland\", \"WinZO\", \"Nazara Games\", \"Netmeds\", \"Practo\", \"PharmEasy\"]\n",
        "                        }\n",
        "\n",
        "        # Track new merchants added in a structured manner\n",
        "        new_merchant_counters = {category: 1 for category in MERCHANT_MAPPING.keys()}  # Start from 1\n",
        "\n",
        "        # Probability of introducing a new merchant\n",
        "        new_merchant_prob = 0.05  # 5% chance\n",
        "\n",
        "        def assign_merchant(product_cd):\n",
        "          if random.random() < new_merchant_prob:\n",
        "            merchant_name = f\"{product_cd}_Merchant{new_merchant_counters[product_cd]:02d}\"  # Example: Wallet_Merchant01\n",
        "            new_merchant_counters[product_cd] += 1  # Increment counter for next merchant\n",
        "            MERCHANT_MAPPING[product_cd].append(merchant_name)  # Store it for future use\n",
        "            return merchant_name\n",
        "          else:\n",
        "            return random.choice(MERCHANT_MAPPING[product_cd])\n",
        "\n",
        "        # Assign Merchant based on ProductCD\n",
        "        transaction[\"Merchant\"] = assign_merchant(transaction[\"ProductCD\"])\n",
        "\n",
        "        def generate_merchant_email(product_category, merchant_name):\n",
        "          \"\"\"Generate an email address in the format category@merchant.com.\"\"\"\n",
        "          category_prefix = transaction['ProductCD'].lower()\n",
        "          domain = merchant_name.lower().replace(\" \", \"\") + \".com\"  # Format domain\n",
        "          return f\"{category_prefix}@{domain}\"\n",
        "\n",
        "        def assign_merchant_email(transaction):\n",
        "            product_category = transaction.get(\"ProductCD\", None)  # Get ProductCD\n",
        "            merchant_name = transaction.get(\"Merchant\", None)  # Get merchant name\n",
        "            if product_category in MERCHANT_MAPPING:\n",
        "                valid_merchants = MERCHANT_MAPPING[product_category]\n",
        "                # Check if the merchant exists in the valid category list\n",
        "                if merchant_name in valid_merchants:\n",
        "                    selected_merchant = merchant_name\n",
        "                else:\n",
        "                    selected_merchant = random.choice(valid_merchants)  # Pick a random one if invalid/missing\n",
        "                transaction[\"Merchant_email\"] = generate_merchant_email(product_category, selected_merchant)\n",
        "            else:\n",
        "                transaction[\"Merchant_email\"] = \"unknown@unknown.com\"  # Fallback for unknown categories\n",
        "\n",
        "            return transaction\n",
        "\n",
        "\n",
        "        assign_merchant_email(transaction)\n",
        "\n",
        "        # Random date in the year 2024\n",
        "        start_date = pd.to_datetime(\"2024-01-01\")\n",
        "        end_date = pd.to_datetime(\"2024-12-31\")\n",
        "        transaction[\"TransactionDT\"] = start_date + (end_date - start_date) * np.random.rand()\n",
        "        transaction[\"TransactionDT\"] = transaction[\"TransactionDT\"].strftime('%Y-%m-%d %H:%M:%S')\n",
        "        data.append(transaction)\n",
        "\n",
        "# Convert list to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# List of realistic DeviceInfo types\n",
        "device_info_list = [\n",
        "    \"Windows\", \"iOS Device\", \"MacOS\", \"Trident/7.0\", \"rv:11.0\", \"rv:57.0\",\n",
        "    \"SM-J700M Build/MMB29K\", \"SM-G610M Build/MMB29K\", \"SM-G531H Build/LMY48B\",\n",
        "    \"rv:59.0\", \"SM-G935F Build/NRD90M\", \"SM-G955U Build/NRD90M\", \"SM-G532M Build/MMB29T\",\n",
        "    \"ALE-L23 Build/HuaweiALE-L23\", \"SM-G950U Build/NRD90M\", \"SM-G930V Build/NRD90M\",\n",
        "    \"rv:58.0\", \"rv:52.0\", \"SAMSUNG\", \"SM-G950F Build/NRD90M\"\n",
        "]\n",
        "\n",
        "# Generate synthetic frequency counts using an exponential decay function (skewed towards common devices)\n",
        "decay_factor = 1.2  # Adjust to control skewness\n",
        "raw_counts = np.exp(-decay_factor * np.arange(len(device_info_list)))\n",
        "raw_counts /= raw_counts.sum()  # Normalize to sum to 1\n",
        "total_transactions = len(df)# Scale counts to match dataset size\n",
        "device_info_counts = {device: int(count * total_transactions) for device, count in zip(device_info_list, raw_counts)}\n",
        "damping_factor = 10 # Apply damping to ensure rare values are not underrepresented\n",
        "device_info_counts = {device: count + damping_factor for device, count in device_info_counts.items()}\n",
        "total_count = sum(device_info_counts.values()) # Normalize to get valid probabilities\n",
        "device_info_probabilities = {device: count / total_count for device, count in device_info_counts.items()}\n",
        "\n",
        "# Assign DeviceInfo based on generated probabilities\n",
        "df[\"DeviceInfo\"] = np.random.choice(\n",
        "    list(device_info_probabilities.keys()),\n",
        "    size=len(df),\n",
        "    p=list(device_info_probabilities.values()))\n",
        "\n",
        "# Infer DeviceType based on DeviceInfo\n",
        "def infer_device_type(device_info):\n",
        "    if \"Windows\" in device_info or \"MacOS\" in device_info or \"rv:\" in device_info:\n",
        "        return \"desktop\"\n",
        "    elif \"SM-\" in device_info or \"SAMSUNG\" in device_info or \"Build/\" in device_info or \"iOS\" in device_info:\n",
        "        return \"mobile\"\n",
        "    else:\n",
        "        return np.random.choice([\"desktop\", \"mobile\"], p=[0.7, 0.3])  # Default skewed towards desktop\n",
        "\n",
        "df[\"DeviceType\"] = df[\"DeviceInfo\"].apply(infer_device_type)"
      ],
      "metadata": {
        "id": "rQCyioaczQNv"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Introduce Missing Values to Match Required Percentages\n",
        "MISSING_PERCENTAGES = {\n",
        "    \"Sender_email\": 16,\n",
        "    \"DeviceType\": 15,\n",
        "    \"DeviceInfo\": 15\n",
        "}\n",
        "\n",
        "for column, percentage in MISSING_PERCENTAGES.items():\n",
        "    num_missing = int(len(df) * (percentage / 100))  # Calculate number of missing values\n",
        "    missing_indices = np.random.choice(df.index, num_missing, replace=False)  # Randomly select indices\n",
        "    df.loc[missing_indices, column] = np.nan  # Assign NaN to selected rows"
      ],
      "metadata": {
        "id": "o-WKyIkMuUoI"
      },
      "execution_count": 133,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**D columns**"
      ],
      "metadata": {
        "id": "yq2z1QLKu5gs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['TransactionDT'] = pd.to_datetime(df['TransactionDT'], format='%Y-%m-%d %H:%M:%S') # Ensure TransactionDT is in datetime format\n",
        "\n",
        "# 'D2'\n",
        "df = df.sort_values(by=['User_ID', 'TransactionDT'])  # Sort the DataFrame by User_ID and TransactionDT before applying diff()\n",
        "df['Days_Since_LastTransac(D2)'] = df.groupby('User_ID')['TransactionDT'].diff().dt.days  # Calculate 'D2' (Days since last transaction per user)\n",
        "df['Days_Since_LastTransac(D2)'] = df['Days_Since_LastTransac(D2)'].clip(lower=0)\n",
        "\n",
        "\n",
        "# 'D3'\n",
        "df = df.sort_values(by=['Card Number', 'TransactionDT'])  # Sort by Card Number and TransactionDT to fix D3\n",
        "df['SameCard_DaysDiff(D3)'] = df.groupby('Card Number')['TransactionDT'].diff().dt.days # Calculate 'D3' (Days difference between transactions for the same card)\n",
        "df['SameCard_DaysDiff(D3)'] = df['SameCard_DaysDiff(D3)'].clip(lower=0) # Replace negative values with 0 (optional, based on business logic)\n",
        "\n",
        "\n",
        "# 'D4'\n",
        "df['SameAddress_DaysDiff(D4)'] = df.groupby('Order_Region')['TransactionDT'].diff().dt.days.apply(lambda x: max(x, 0))  # Calculate D4 (Time difference between current and last transaction at the same billing address)\n",
        "\n",
        "\n",
        "# 'D10'\n",
        "df['SameReceiverEmail_DaysDiff(D10)'] = df.groupby('Merchant_email')['TransactionDT'].diff().dt.days.apply(lambda x: max(x, 0)) # Calculate D10 (Time difference between transactions for the same receiver email domain)\n",
        "\n",
        "\n",
        "# 'D11'\n",
        "df['SameDeviceType_DaysDiff(D11)'] = df.groupby('DeviceType')['TransactionDT'].diff().dt.days.apply(lambda x: max(x, 0))  # Calculate D11 (Time difference between transactions for a specific device type)"
      ],
      "metadata": {
        "id": "MB5I0F2-u4Ki"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**M columns**"
      ],
      "metadata": {
        "id": "jH2HmKPOu9DV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# M4\n",
        "# Find the most used DeviceType and DeviceInfo for each user\n",
        "most_used_device_info = df.groupby('User_ID').agg({\n",
        "    'DeviceType': lambda x: x.mode()[0] if not x.mode().empty else 'Unknown',  # Most frequent DeviceType, default to 'Unknown' if empty\n",
        "    'DeviceInfo': lambda x: x.mode()[0] if not x.mode().empty else 'Unknown'   # Most frequent DeviceInfo, default to 'Unknown' if empty\n",
        "}).reset_index()\n",
        "\n",
        "df = pd.merge(df, most_used_device_info, on='User_ID', how='left', suffixes=('', '_MostUsed')) # Merge the most used device information back to the original dataframe\n",
        "\n",
        "# Create the 'Device Matching(M4)' column to indicate whether both DeviceType and DeviceInfo match\n",
        "df['Device Matching(M4)'] = (\n",
        "    (df['DeviceType'] == df['DeviceType_MostUsed']) &\n",
        "    (df['DeviceInfo'] == df['DeviceInfo_MostUsed'])\n",
        ").astype(int)\n",
        "df = df.drop(columns=['DeviceType_MostUsed', 'DeviceInfo_MostUsed'])  # Drop the temporary columns used for matching\n",
        "\n",
        "\n",
        "# M6\n",
        "# For 'DeviceMismatch' column, we need to check if the device used in a transaction is different from the most frequent device for that user.\n",
        "# First, find the most frequently used device for each user (User_ID)\n",
        "most_used_device = df.groupby('User_ID')['DeviceType'].agg(\n",
        "    lambda x: x.mode()[0] if not x.mode().empty else 'Unknown'  # Handle empty mode by setting 'Unknown'\n",
        ").reset_index()\n",
        "most_used_device.columns = ['User_ID', 'MostUsedDevice']\n",
        "df = pd.merge(df, most_used_device, on='User_ID', how='left') # Merge the most used device information back to the original dataframe\n",
        "\n",
        "# Create the 'Device Mismatch(M6)' column to indicate whether the device used in the transaction is different from the most frequent device for that user\n",
        "df['Device Mismatch(M6)'] = (df['DeviceType'] != df['MostUsedDevice']).astype(int)\n",
        "df = df.drop(columns=['MostUsedDevice'])  # Drop 'MostUsedDevice' as it's no longer needed"
      ],
      "metadata": {
        "id": "u9Dcs0zc0F7n"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# M8\n",
        "# Function to calculate geodesic distance between User_Region and Receiver_Region\n",
        "def calculate_user_order_distance(user_region, receiver_region, threshold_km=40):\n",
        "    user_coords = BENGALURU_REGIONS.get(user_region)\n",
        "    receiver_coords = BENGALURU_REGIONS.get(receiver_region)\n",
        "    if user_coords is None or receiver_coords is None:  # If any of the regions are missing or invalid, return NaN (indicating missing data)\n",
        "        return np.nan\n",
        "    distance = np.round(geodesic(user_coords, receiver_coords).km, 2) # Calculate geodesic distance\n",
        "    return 1 if distance > threshold_km else 0  # Flag as mismatch (1) if distance exceeds the threshold, otherwise no mismatch (0)\n",
        "\n",
        "# Apply function to DataFrame to get Region Mismatch (M8)\n",
        "df['RegionMismatch(M8)'] = df.apply(lambda x: calculate_user_order_distance(x['User_Region'], x['Receiver_Region']), axis=1)\n",
        "\n",
        "\n",
        "# M9\n",
        "\n",
        "df['TransactionDT'] = pd.to_datetime(df['TransactionDT']) # Ensure 'TransactionDT' is a datetime object\n",
        "df['_TransactionDT_numeric'] = (df['TransactionDT'] - df['TransactionDT'].min()).dt.total_seconds() # Create a temporary numeric column for transaction time (convert to seconds)\n",
        "# Identify the most common value for each feature per user\n",
        "user_patterns = df.groupby('User_ID').agg({\n",
        "    'TransactionAmt': 'mean',                                                 # The typical transaction amount (mean or mode)\n",
        "    'ProductCD': lambda x: x.mode()[0] if not x.mode().empty else 'Unknown',  # Most common product category\n",
        "    'DeviceType': lambda x: x.mode()[0] if not x.mode().empty else 'Unknown', # Most common device type\n",
        "    'DeviceInfo': lambda x: x.mode()[0] if not x.mode().empty else 'Unknown', # Most common device info\n",
        "    '_TransactionDT_numeric': 'mean'                                          # Typical transaction time in seconds\n",
        "}).reset_index()\n",
        "user_patterns.rename(columns={'_TransactionDT_numeric': 'TransactionDT_mode'}, inplace=True)  # Rename temporary column in user_patterns\n",
        "df = df.merge(user_patterns, on='User_ID', suffixes=('', '_mode'))  # Merge these user patterns back into the main dataset\n",
        "\n",
        "# Define thresholds to flag inconsistency\n",
        "threshold_amount = 0.2  # Example: If the transaction amount deviates by 20% from the mean, mark as inconsistent\n",
        "threshold_time = 6 * 3600  # 6 hours in seconds\n",
        "\n",
        "# Flag deviations in transaction parameters\n",
        "df['TransactionConsistency(M9)'] = (\n",
        "    (abs(df['TransactionAmt'] - df['TransactionAmt_mode']) > threshold_amount * df['TransactionAmt_mode']) &\n",
        "    (df['ProductCD'] != df['ProductCD_mode']) &\n",
        "    (df['DeviceType'] != df['DeviceType_mode']) &\n",
        "    (df['DeviceInfo'] != df['DeviceInfo_mode']) &\n",
        "    (abs(df['_TransactionDT_numeric'] - df['TransactionDT_mode']) > threshold_time)  # Time inconsistency check\n",
        ").astype(int)  # 1 if inconsistent, 0 if consistent\n",
        "\n",
        "df.drop(columns=['_TransactionDT_numeric', 'TransactionAmt_mode', 'ProductCD_mode', 'TransactionDT_mode',\n",
        "                 'DeviceType_mode', 'DeviceInfo_mode'], inplace=True) # Drop the temporary column"
      ],
      "metadata": {
        "id": "qBC2gg9LzCiH"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**C columns**"
      ],
      "metadata": {
        "id": "06tLDP-v-GPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# C1\n",
        "# Count transactions associated with the same card1 and order region\n",
        "df['Transaction Count(card,U_Region C1)'] = df.groupby(['Card Number', 'Order_Region'])['TransactionID'].transform('count')\n",
        "\n",
        "# C4\n",
        "# Count unique merchants per Card Number (C4)\n",
        "df['Unique Merchants(per card C4)'] = df.groupby('Card Number')['Merchant'].transform('nunique')\n",
        "\n",
        "# C5\n",
        "# Count transactions linked to the same Billing Region per User_ID\n",
        "df['Same B_region count(C5)'] = df.groupby(['User_ID', 'Order_Region'])['TransactionID'].transform('count')\n",
        "\n",
        "# C6\n",
        "# Count transactions from the same Device per User_ID\n",
        "df['Same Device count(C6)'] = df.groupby(['User_ID', 'DeviceType', 'DeviceInfo'])['TransactionID'].transform('count')\n",
        "\n",
        "# C11\n",
        "# Count of unique Billing Regions linked to the same Card Number\n",
        "df['Unique B_region(same card C11)'] = df.groupby('Card Number')['Order_Region'].transform('nunique')\n",
        "\n",
        "\n",
        "df['TransactionDT'] = pd.to_datetime(df['TransactionDT'], format='%Y-%m-%d %H:%M:%S') # Ensure 'TransactionDT' is in datetime format\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "desired_order = [\n",
        "                'TransactionID', 'TransactionAmt', 'TransactionDT', 'ProductCD', 'User_ID', 'Merchant',\n",
        "                'Card Number', 'BIN Number', 'Card Network', 'Card Tier', 'Card Type','Phone Numbers',\n",
        "                'User_Region', 'Order_Region','Receiver_Region', 'Distance', 'Sender_email', 'Merchant_email',\n",
        "                'DeviceType', 'DeviceInfo', 'Days_Since_LastTransac(D2)','SameCard_DaysDiff(D3)',\n",
        "                'SameAddress_DaysDiff(D4)', 'SameReceiverEmail_DaysDiff(D10)','SameDeviceType_DaysDiff(D11)',\n",
        "                'Device Matching(M4)', 'Device Mismatch(M6)','RegionMismatch(M8)', 'TransactionConsistency(M9)',\n",
        "                'Transaction Count(card,U_Region C1)','Unique Merchants(per card C4)', 'Same B_region count(C5)',\n",
        "                'Same Device count(C6)', 'Unique B_region(same card C11)', 'isFraud']\n",
        "\n",
        "# Reorder DataFrame columns\n",
        "df = df[desired_order]"
      ],
      "metadata": {
        "id": "oVb29mRfvLSn"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgWhnjDFUSrj",
        "outputId": "cc944d13-cc02-4b37-9522-ffbf9001b5c1",
        "collapsed": true
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'TransactionAmt', 'TransactionDT', 'ProductCD',\n",
              "       'User_ID', 'Merchant', 'Card Number', 'BIN Number', 'Card Network',\n",
              "       'Card Tier', 'Card Type', 'Phone Numbers', 'User_Region',\n",
              "       'Order_Region', 'Receiver_Region', 'Distance', 'Sender_email',\n",
              "       'Merchant_email', 'DeviceType', 'DeviceInfo',\n",
              "       'Days_Since_LastTransac(D2)', 'SameCard_DaysDiff(D3)',\n",
              "       'SameAddress_DaysDiff(D4)', 'SameReceiverEmail_DaysDiff(D10)',\n",
              "       'SameDeviceType_DaysDiff(D11)', 'Device Matching(M4)',\n",
              "       'Device Mismatch(M6)', 'RegionMismatch(M8)',\n",
              "       'TransactionConsistency(M9)', 'Transaction Count(card,U_Region C1)',\n",
              "       'Unique Merchants(per card C4)', 'Same B_region count(C5)',\n",
              "       'Same Device count(C6)', 'Unique B_region(same card C11)', 'isFraud'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-63tuuu9iWT",
        "outputId": "61b5f4b0-de8f-4656-c865-1f12c4dd0c7b"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"TransactionAmt\"] = df[\"TransactionAmt\"].apply(lambda x: round(x, 2))\n",
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2tFQlDJlbIs",
        "outputId": "5012b5d2-8550-428b-b20f-6033e3f75700"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9954, 35)"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['TransactionDT'] = pd.to_datetime(df['TransactionDT'], format='%Y-%m-%d %H:%M:%S') # Ensure 'TransactionDT' is in datetime format\n",
        "\n",
        "\n",
        "# E2 - Time Range Encoding (Time of Day)\n",
        "def time_to_range(dt):\n",
        "    hour = dt.hour\n",
        "    return (0 if 10 <= hour < 14 else\n",
        "            1 if 14 <= hour < 18 else\n",
        "            2 if 18 <= hour < 22 else\n",
        "            3 if hour >= 22 or hour < 2 else\n",
        "            4 if 2 <= hour < 6 else 5)\n",
        "df['TransactionTimeSlot(E2)'] = df['TransactionDT'].apply(time_to_range)\n",
        "\n",
        "\n",
        "# E3 - Hour Difference from Range Start\n",
        "def time_to_range12(dt):\n",
        "    hour = dt.hour\n",
        "    return (hour - 10 if 10 <= hour < 14 else\n",
        "            hour - 14 if 14 <= hour < 18 else\n",
        "            hour - 18 if 18 <= hour < 22 else\n",
        "            (hour - 22) if hour >= 22 else (hour + 2) if hour < 2 else\n",
        "            hour - 2 if 2 <= hour < 6 else hour - 6)\n",
        "df['HourWithinSlot(E3)'] = df['TransactionDT'].apply(time_to_range12)\n",
        "\n",
        "\n",
        "# E4 - Day of the Week\n",
        "df['TransactionWeekday(E4)'] = df['TransactionDT'].dt.weekday\n",
        "\n",
        "\n",
        "# E5 - Average Transaction Interval\n",
        "df['TimeDiff'] = df.groupby('Card Number')['TransactionDT'].diff().dt.total_seconds() / 60\n",
        "df['AvgTransactionInterval(E5)'] = df.groupby('Card Number')['TimeDiff'].transform('mean')\n",
        "df['AvgTransactionInterval(E5)'] = pd.cut(df['AvgTransactionInterval(E5)'], bins=[-np.inf, df['AvgTransactionInterval(E5)'].quantile(0.33), df['AvgTransactionInterval(E5)'].quantile(0.66), np.inf], labels=['0', '1', '2'], right=False)\n",
        "df.drop(columns=['TimeDiff'], inplace=True)\n",
        "\n",
        "\n",
        "# E6 - Transaction Amount Variance\n",
        "df['TransactionAmountVariance(E6)'] = df.groupby('Card Number')['TransactionAmt'].transform(lambda x: x.var())\n",
        "df['TransactionAmountVariance(E6)'] = pd.qcut(df['TransactionAmountVariance(E6)'], q=10, labels=range(10), duplicates='drop')\n",
        "\n",
        "\n",
        "# E7 - Transaction Amount Ratio\n",
        "df['AvgTransactionAmount'] = df.groupby('Card Number')['TransactionAmt'].transform('mean')\n",
        "df['TransactionRatio(E7)'] = (df['TransactionAmt'] / df['AvgTransactionAmount']).replace([np.inf, -np.inf], np.nan).fillna(0)\n",
        "df['TransactionRatio(E7)'] = pd.qcut(df['TransactionRatio(E7)'], q=7, labels=range(7), duplicates='drop')\n",
        "df.drop(columns=['AvgTransactionAmount'], inplace=True)\n",
        "\n",
        "\n",
        "# E8 - Median Transaction Amount\n",
        "df['MedianTransactionAmount(E8)'] = df.groupby('Card Number')['TransactionAmt'].transform('median')\n",
        "df['MedianTransactionAmount(E8)'] = pd.qcut(df['MedianTransactionAmount(E8)'], q=9, labels=range(9), duplicates='drop')\n",
        "\n",
        "\n",
        "# E11\n",
        "def add_anomaly_group(df, device_col='DeviceType', dist_col='Distance', time_col='TransactionDT', speed_kmh=10, user_col='Order_Region'):\n",
        "    df = df.copy()  # Ensure the original DataFrame is not modified unintentionally\n",
        "    df.columns = df.columns.str.strip()  # Clean up column names to remove any extra spaces\n",
        "    df[time_col] = pd.to_datetime(df[time_col])\n",
        "    df = df.sort_values(by=[device_col, time_col])\n",
        "    df['PrevTime'] = df.groupby(device_col)[time_col].shift(1)\n",
        "    df['TimeDiffHours'] = (df[time_col] - df['PrevTime']).dt.total_seconds() / 3600 # Calculate the time difference between consecutive transactions in hours\n",
        "    df['ExpectedTimeHours'] = df[dist_col] / speed_kmh  # Calculate the expected time to travel the given distance at the specified speed\n",
        "    df['TimingAnomaly'] = df['TimeDiffHours'] < df['ExpectedTimeHours'] # Detect anomalies: mark as True if the time difference is less than the expected time\n",
        "    df['Timing Anomaly(E11)'] = df['TimingAnomaly'].astype(int) # Create the Anomaly Group (1 for anomaly, 0 for no anomaly)\n",
        "    df.drop(columns=['PrevTime', 'TimeDiffHours', 'ExpectedTimeHours'], inplace=True) # Drop temporary columns, as only the final anomaly-related columns are needed\n",
        "\n",
        "    return df\n",
        "\n",
        "df = add_anomaly_group(df)\n",
        "\n",
        "\n",
        "# E12\n",
        "def add_region_anomaly_v34(df, sender_col='Order_Region', receiver_col='Receiver_Region', dist_col='Distance', time_col='TransactionDT', speed_kmh=30):\n",
        "    df = df.copy()\n",
        "    df[time_col] = pd.to_datetime(df[time_col])\n",
        "    df = df.sort_values(by=[sender_col, time_col])\n",
        "    prev_time = df.groupby(sender_col)[time_col].shift(1)\n",
        "    time_diff_hours = (df[time_col] - prev_time).dt.total_seconds() / 3600\n",
        "    expected_time_hours = df[dist_col] / speed_kmh\n",
        "    df['Region Anomaly(E12)'] = (time_diff_hours < expected_time_hours).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = add_region_anomaly_v34(df)\n",
        "\n",
        "\n",
        "\n",
        "#E13\n",
        "def add_hourly_transaction_count(df, time_col=\"TransactionDT\", group_col=\"Merchant_email\"):\n",
        "    df = df.copy()\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df[time_col]):     # Ensure TransactionDT is in datetime format\n",
        "        df[time_col] = pd.to_datetime(df[time_col])\n",
        "    df[\"Receiver_emaildomain\"] = df[group_col].str.split('@').str[1]  # Extract domain from the Receiver_email\n",
        "    df[\"Receiver_emaildomain\"] = df[\"Receiver_emaildomain\"].fillna(\"Unknown\")     # Handle missing or malformed email addresses\n",
        "    df[\"HourlyTransactionCount(E13)\"] = (df.groupby([pd.Grouper(key=time_col, freq=\"1h\"), \"Receiver_emaildomain\"])\n",
        "        [\"Receiver_emaildomain\"].transform(\"count\"))  # Group transactions by hour and count occurrences\n",
        "    df = df.drop(columns=[\"Receiver_emaildomain\"])\n",
        "\n",
        "    return df\n",
        "\n",
        "df = add_hourly_transaction_count(df)\n",
        "df = df.sort_values(by=\"TransactionDT\")"
      ],
      "metadata": {
        "id": "WxkTz4mfQm9C",
        "collapsed": true
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#E9\n",
        "\n",
        "def add_avg_transaction_amt_24hrs(df, group_col='Card Number', amount_col='TransactionAmt', time_col='TransactionDT'):\n",
        "    df = df.copy()\n",
        "    df[time_col] = pd.to_datetime(df[time_col])\n",
        "    df = df.sort_values(by=[group_col, time_col])\n",
        "    df['Last24hTransactionSum'] = 0.0   # Initialize new column\n",
        "    for card, group in df.groupby(group_col): # Compute last 24-hour transaction sum per group\n",
        "        sums = []\n",
        "        for i, row in group.iterrows():\n",
        "            time_threshold = row[time_col] - pd.Timedelta(hours=24)\n",
        "            last_24h_sum = group[(group[time_col] >= time_threshold) & (group[time_col] <= row[time_col])][amount_col].sum()\n",
        "            sums.append(last_24h_sum)\n",
        "        df.loc[group.index, 'Last24hTransactionSum'] = sums\n",
        "    df['Last24hTransactionSum'] = df['Last24hTransactionSum'].fillna(0)\n",
        "    df['AvgTransactionAmt_24Hrs(E9)'] = pd.qcut(df['Last24hTransactionSum'], q=3, labels=[0, 1, 2], duplicates='drop') # Categorizing into bins\n",
        "    return df.drop(columns=['Last24hTransactionSum']) # Drop intermediate column\n",
        "\n",
        "df = add_avg_transaction_amt_24hrs(df)\n",
        "\n",
        "\n",
        "#E10\n",
        "\n",
        "def add_transaction_velocity(df, time_col='TransactionDT', id_col='Card Number'):\n",
        "    df = df.copy()\n",
        "    df[time_col] = pd.to_datetime(df[time_col], errors='coerce')\n",
        "    if df[time_col].isna().any():   # Check for invalid datetime values\n",
        "        raise ValueError(\"Some values in TransactionDT could not be converted to datetime.\")\n",
        "    df = df.sort_values(by=[id_col, time_col])\n",
        "    df['Transaction Velocity(E10)'] = 0     # Initialize new column\n",
        "    for card, group in df.groupby(id_col):        # Compute transaction count in the last 1 hour per group\n",
        "        counts = []\n",
        "        for i, row in group.iterrows():\n",
        "            time_threshold = row[time_col] - pd.Timedelta(hours=1)\n",
        "            last_hour_count = group[(group[time_col] >= time_threshold) & (group[time_col] < row[time_col])].shape[0]\n",
        "            counts.append(last_hour_count)\n",
        "        df.loc[group.index, 'Transaction Velocity(E10)'] = counts\n",
        "\n",
        "    return df\n",
        "\n",
        "df = add_transaction_velocity(df)"
      ],
      "metadata": {
        "id": "5pHmNxe2lziS"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTv4hswemarq",
        "outputId": "659452f1-609f-481d-a37d-e8bad3a7f998"
      },
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['TransactionID', 'TransactionAmt', 'TransactionDT', 'ProductCD',\n",
              "       'User_ID', 'Merchant', 'Card Number', 'BIN Number', 'Card Network',\n",
              "       'Card Tier', 'Card Type', 'Phone Numbers', 'User_Region',\n",
              "       'Order_Region', 'Receiver_Region', 'Distance', 'Sender_email',\n",
              "       'Merchant_email', 'DeviceType', 'DeviceInfo',\n",
              "       'Days_Since_LastTransac(D2)', 'SameCard_DaysDiff(D3)',\n",
              "       'SameAddress_DaysDiff(D4)', 'SameReceiverEmail_DaysDiff(D10)',\n",
              "       'SameDeviceType_DaysDiff(D11)', 'Device Matching(M4)',\n",
              "       'Device Mismatch(M6)', 'RegionMismatch(M8)',\n",
              "       'TransactionConsistency(M9)', 'Transaction Count(card,U_Region C1)',\n",
              "       'Unique Merchants(per card C4)', 'Same B_region count(C5)',\n",
              "       'Same Device count(C6)', 'Unique B_region(same card C11)', 'isFraud',\n",
              "       'TransactionTimeSlot(E2)', 'HourWithinSlot(E3)',\n",
              "       'TransactionWeekday(E4)', 'AvgTransactionInterval(E5)',\n",
              "       'TransactionAmountVariance(E6)', 'TransactionRatio(E7)',\n",
              "       'MedianTransactionAmount(E8)', 'TimingAnomaly', 'Timing Anomaly(E11)',\n",
              "       'Region Anomaly(E12)', 'HourlyTransactionCount(E13)',\n",
              "       'AvgTransactionAmt_24Hrs(E9)', 'Transaction Velocity(E10)'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAf3mc-L4n5d",
        "outputId": "33c37572-bd56-4e8e-d66a-a5b4687e786e"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9954, 48)"
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "desired_order = [\n",
        "                'TransactionID', 'TransactionAmt', 'TransactionDT', 'ProductCD', 'User_ID', 'Merchant','Card Number', 'BIN Number',\n",
        "                'Card Network', 'Card Tier', 'Card Type','Phone Numbers', 'User_Region', 'Order_Region','Receiver_Region', 'Distance',\n",
        "                'Sender_email', 'Merchant_email','DeviceType', 'DeviceInfo', 'Days_Since_LastTransac(D2)','SameCard_DaysDiff(D3)',\n",
        "                'SameAddress_DaysDiff(D4)', 'SameReceiverEmail_DaysDiff(D10)','SameDeviceType_DaysDiff(D11)', 'Device Matching(M4)',\n",
        "                'Device Mismatch(M6)', 'RegionMismatch(M8)', 'TransactionConsistency(M9)', 'Transaction Count(card,U_Region C1)',\n",
        "                'Unique Merchants(per card C4)', 'Same B_region count(C5)','Same Device count(C6)', 'Unique B_region(same card C11)',\n",
        "                'TransactionTimeSlot(E2)','HourWithinSlot(E3)','TransactionWeekday(E4)','AvgTransactionInterval(E5)',\n",
        "                'TransactionAmountVariance(E6)','TransactionRatio(E7)','MedianTransactionAmount(E8)','AvgTransactionAmt_24Hrs(E9)',\n",
        "                'Transaction Velocity(E10)','Timing Anomaly(E11)', 'Region Anomaly(E12)','HourlyTransactionCount(E13)','isFraud'\n",
        "]\n",
        "\n",
        "df = df[desired_order]"
      ],
      "metadata": {
        "id": "i3k_c_XAQuiC"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_fraud_label(row, fraud_probabilities, df_grouped_by_user):\n",
        "    base_prob = FRAUD_RATIO\n",
        "    final_prob = base_prob\n",
        "\n",
        "    user_id = row['User_ID']\n",
        "\n",
        "    try:\n",
        "        user_data = df_grouped_by_user.get_group(user_id)\n",
        "        user_avg_amt = user_data['TransactionAmt'].mean()\n",
        "\n",
        "        current_time = row['TransactionDT']\n",
        "        last_30_sec_transactions = user_data[user_data['TransactionDT'] >= current_time - pd.Timedelta(seconds=30)]\n",
        "        num_transactions_last_30_sec = len(last_30_sec_transactions)\n",
        "\n",
        "        # Calculate temporary AvgTransactionAmt_24Hrs (if it doesn't exist in the ROW)\n",
        "        if \"AvgTransactionAmt_24Hrs\" not in row:  # Check the ROW, not the DataFrame\n",
        "            user_data_24hrs = user_data[user_data['TransactionDT'] >= current_time - pd.Timedelta(hours=24)]\n",
        "            row['AvgTransactionAmt_24Hrs'] = user_data_24hrs['TransactionAmt'].mean() if len(user_data_24hrs)>0 else 0\n",
        "\n",
        "        # Calculate temporary Avg_TransactionAmt (if it doesn't exist in the ROW)\n",
        "        if \"Avg_TransactionAmt\" not in row:  # Check the ROW, not the DataFrame\n",
        "            row['Avg_TransactionAmt'] = user_avg_amt\n",
        "\n",
        "    except KeyError:\n",
        "        user_avg_amt = row['TransactionAmt']\n",
        "        num_transactions_last_30_sec = 0\n",
        "        row['AvgTransactionAmt_24Hrs'] = 0\n",
        "        row['Avg_TransactionAmt'] = 0\n",
        "\n",
        "    # New Rule 1\n",
        "    if (row[\"Days_Since_LastTransac(D2)\"] > 30 and\n",
        "        row[\"Device Matching(M4)\"] == 0 and  # Use the renamed column\n",
        "        row[\"TransactionAmt\"] > 3 * user_avg_amt and\n",
        "        (user_data['TransactionAmt'] > 3* user_data['TransactionAmt'].mean()).sum() > 0 and\n",
        "        (row[\"AvgTransactionAmt_24Hrs\"] < row[\"Avg_TransactionAmt\"] or row[\"RegionMismatch(M8)\"] == 1)):  # Corrected condition\n",
        "        final_prob = max(final_prob, 0.02)\n",
        "\n",
        "    # New Rule 2\n",
        "    if (row[\"Days_Since_LastTransac(D2)\"] < 1 and\n",
        "        row[\"TransactionAmt\"] > 3 * user_avg_amt and\n",
        "        num_transactions_last_30_sec > 3 and\n",
        "        row[\"Device Mismatch(M6)\"] == 1 and\n",
        "        row[\"RegionMismatch(M8)\"] == 1):\n",
        "        final_prob = max(final_prob, 0.05)\n",
        "\n",
        "    # New Rule 3\n",
        "    if (row['ProductCD'] in [\"Household\", \"Consumable\", \"Services\"] and\n",
        "        row['Device Mismatch(M6)'] == 1 and\n",
        "        row['RegionMismatch(M8)'] == 1 and row['TransactionAmt'] > 3*row['Avg_TransactionAmt']):\n",
        "        final_prob = max(final_prob, 0.34)\n",
        "\n",
        "\n",
        "\n",
        "    if row[\"Card Network\"] == \"Americanexpress\" and row[\"Card Type\"] == \"credit\":\n",
        "        final_prob = max(final_prob, 0.09)\n",
        "    elif row[\"ProductCD\"] == \"Consumable\" and row[\"Card Network\"] == \"Mastercard\" and row[\"Card Type\"] == \"debit\":\n",
        "        final_prob = max(final_prob, 0.02)\n",
        "    elif row[\"TransactionAmt\"] > 10000:\n",
        "        final_prob = max(final_prob, 0.015)\n",
        "\n",
        "    # Probabilistic Adjustments\n",
        "    for feature, probabilities in fraud_probabilities.items():\n",
        "        if feature in row and str(row[feature]) in probabilities:\n",
        "            feature_prob = probabilities[str(row[feature])]\n",
        "            final_prob = max(final_prob, feature_prob)\n",
        "\n",
        "    # M Feature Influence\n",
        "    m_score = 0\n",
        "    if row[\"Device Mismatch(M6)\"] == 1:\n",
        "        m_score += 0.02\n",
        "    if row[\"RegionMismatch(M8)\"] == 1:\n",
        "        m_score += 0.015\n",
        "    if row[\"TransactionConsistency(M9)\"] == 1:\n",
        "        m_score += 0.01\n",
        "\n",
        "    final_prob = min(1, final_prob + m_score)\n",
        "\n",
        "    # Delete the temporary columns AFTER they are used\n",
        "    if \"AvgTransactionAmt_24Hrs\" in row:\n",
        "        del row['AvgTransactionAmt_24Hrs']\n",
        "    if \"Avg_TransactionAmt\" in row:\n",
        "        del row['Avg_TransactionAmt']\n",
        "\n",
        "    return np.random.choice([0, 1], p=[1 - final_prob, final_prob])\n",
        "\n",
        "\n",
        "def calculate_fraud_probabilities(df, features):\n",
        "    fraud_probabilities = {}\n",
        "    for feature in features:\n",
        "        try:\n",
        "            probabilities = df.groupby(feature)[\"isFraud\"].mean().to_dict()\n",
        "            string_probabilities = {str(k): v for k, v in probabilities.items()}\n",
        "            fraud_probabilities[feature] = string_probabilities\n",
        "        except (KeyError, TypeError) as e:\n",
        "            print(f\"Warning: Could not calculate probabilities for {feature}. Error: {e}\")\n",
        "            default_prob = df[\"isFraud\"].mean() if \"isFraud\" in df else FRAUD_RATIO\n",
        "            fraud_probabilities[feature] = {str(val): default_prob for val in df[feature].unique()}\n",
        "    return fraud_probabilities\n",
        "\n",
        "\n",
        "# --- Main Execution ---\n",
        "FRAUD_RATIO = 0.04\n",
        "\n",
        "features_for_prob = [\"Card Network\", \"Card Type\", \"ProductCD\", \"Order_Region\", \"DeviceType\"]\n",
        "fraud_probabilities = calculate_fraud_probabilities(df, features_for_prob)\n",
        "\n",
        "df_grouped_by_user = df.groupby('User_ID')\n",
        "\n",
        "df[\"isFraud\"] = df.apply(assign_fraud_label, axis=1, args=(fraud_probabilities, df_grouped_by_user))"
      ],
      "metadata": {
        "id": "mlnZpknRzrpx"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sort_values(by=\"TransactionDT\")\n"
      ],
      "metadata": {
        "id": "j9OolsYb0PKx"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df.columns))"
      ],
      "metadata": {
        "id": "yafB36uj36wG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a56cc46d-1829-4c2d-ce65-a82c68533a9c"
      },
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['isFraud'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "FEo90jNKz0F8",
        "outputId": "75805a6e-f908-4d55-b755-66187334305f"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "isFraud\n",
              "0    9491\n",
              "1     463\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>isFraud</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9491</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>463</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to Duplicate the transctions (30% legit and 40% fraud)\n",
        "num_legit_repeats = int(0.3 * len(df[df['isFraud'] == 0]))\n",
        "num_fraud_repeats = int(0.4 * len(df[df['isFraud'] == 1]))\n",
        "\n",
        "repeat_legit = df[df['isFraud'] == 0].sample(num_legit_repeats, replace=True).copy()\n",
        "repeat_fraud = df[df['isFraud'] == 1].sample(num_fraud_repeats, replace=True).copy()\n",
        "\n",
        "# *** KEY CHANGE: Generate new TransactionIDs ***\n",
        "new_legit_ids = range(df['TransactionID'].max() + 1, df['TransactionID'].max() + 1 + num_legit_repeats)\n",
        "repeat_legit['TransactionID'] = new_legit_ids\n",
        "\n",
        "new_fraud_ids = range(repeat_legit['TransactionID'].max() + 1, repeat_legit['TransactionID'].max() + 1 + num_fraud_repeats)\n",
        "repeat_fraud['TransactionID'] = new_fraud_ids\n",
        "\n",
        "\n",
        "# Modify timestamps, amounts, and shuffle device info (as before)\n",
        "repeat_legit['TransactionDT'] += pd.to_timedelta(np.random.randint(10, 1440, num_legit_repeats), unit='m')\n",
        "repeat_fraud['TransactionDT'] += pd.to_timedelta(np.random.randint(1, 60, num_fraud_repeats), unit='m')\n",
        "\n",
        "repeat_legit['TransactionAmt'] *= np.random.uniform(0.98, 1.02, num_legit_repeats)\n",
        "repeat_fraud['TransactionAmt'] *= np.random.uniform(0.95, 1.05, num_fraud_repeats)\n",
        "\n",
        "repeat_fraud['DeviceInfo'] = repeat_fraud['DeviceInfo'].sample(frac=1).values\n",
        "\n",
        "# Ensure same columns\n",
        "repeat_legit = repeat_legit[df.columns]\n",
        "repeat_fraud = repeat_fraud[df.columns]\n",
        "\n",
        "# Concatenate\n",
        "df = pd.concat([df, repeat_legit, repeat_fraud], ignore_index=True)\n",
        "\n",
        "# Sort by TransactionDT\n",
        "df = df.sort_values(by=\"TransactionDT\")\n"
      ],
      "metadata": {
        "id": "XM7wbRaU_jnn"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['isFraud'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "NGu8vOsOKKWd",
        "outputId": "f27c82ca-7ce1-4374-ac97-9fa4f490328d"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "isFraud\n",
              "0    12338\n",
              "1      648\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>isFraud</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>648</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Original column names\n",
        "old_columns = ['TransactionID', 'TransactionAmt', 'TransactionDT', 'ProductCD',\n",
        "       'User_ID', 'Merchant', 'Card Number', 'BIN Number', 'Card Network',\n",
        "       'Card Tier', 'Card Type', 'Phone Numbers', 'User_Region',\n",
        "       'Order_Region', 'Receiver_Region', 'Distance', 'Sender_email',\n",
        "       'Merchant_email', 'DeviceType', 'DeviceInfo',\n",
        "       'Days_Since_LastTransac(D2)', 'SameCard_DaysDiff(D3)',\n",
        "       'SameAddress_DaysDiff(D4)', 'SameReceiverEmail_DaysDiff(D10)',\n",
        "       'SameDeviceType_DaysDiff(D11)', 'Device Matching(M4)',\n",
        "       'Device Mismatch(M6)', 'RegionMismatch(M8)',\n",
        "       'TransactionConsistency(M9)', 'Transaction Count(card,U_Region C1)',\n",
        "       'Unique Merchants(per card C4)', 'Same B_region count(C5)',\n",
        "       'Same Device count(C6)', 'Unique B_region(same card C11)',\n",
        "       'TransactionTimeSlot(E2)', 'HourWithinSlot(E3)',\n",
        "       'TransactionWeekday(E4)', 'AvgTransactionInterval(E5)',\n",
        "       'TransactionAmountVariance(E6)', 'TransactionRatio(E7)',\n",
        "       'MedianTransactionAmount(E8)', 'AvgTransactionAmt_24Hrs(E9)',\n",
        "       'Transaction Velocity(E10)', 'Timing Anomaly(E11)',\n",
        "       'Region Anomaly(E12)', 'HourlyTransactionCount(E13)', 'isFraud']\n",
        "\n",
        "# New column names\n",
        "new_columns = ['TransactionID', 'TransactionAmt', 'TransactionDT', 'ProductCD',\n",
        "       'User_ID', 'Merchant', 'Card_Number', 'BIN_Number', 'Card_Network',\n",
        "       'Card_Tier', 'Card_Type', 'Phone_Numbers', 'User_Region',\n",
        "       'Order_Region', 'Receiver_Region', 'Distance', 'Sender_email', 'Merchant_email',\n",
        "       'DeviceType', 'DeviceInfo', 'Days_Since_LastTransac_D2',\n",
        "       'SameCard_DaysDiff_D3', 'SameAddress_DaysDiff_D4',\n",
        "       'SameReceiverEmail_DaysDiff_D10', 'SameDeviceType_DaysDiff_D11',\n",
        "       'Device_Matching_M4', 'Device_Mismatch_M6', 'RegionMismatch_M8',\n",
        "       'TransactionConsistency_M9', 'Transaction_Count_C1',\n",
        "       'Unique_Merchants_C4', 'Same_B_region_count_C5', 'Same_Device_count_C6',\n",
        "       'Unique_B_region_C11', 'TransactionTimeSlot_E2', 'HourWithinSlot_E3',\n",
        "       'TransactionWeekday_E4', 'AvgTransactionInterval_E5',\n",
        "       'TransactionAmountVariance_E6', 'TransactionRatio_E7',\n",
        "       'MedianTransactionAmount_E8', 'AvgTransactionAmt_24Hrs_E9',\n",
        "       'TransactionVelocity_E10', 'TimingAnomaly_E11', 'RegionAnomaly_E12',\n",
        "       'HourlyTransactionCount_E13', 'isFraud']\n",
        "\n",
        "# Create a renaming dictionary\n",
        "rename_dict = dict(zip(old_columns, new_columns))\n",
        "\n",
        "# Assuming df is your DataFrame\n",
        "df.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "df[\"TransactionAmt\"] = df[\"TransactionAmt\"].apply(lambda x: round(x, 2))\n",
        "\n",
        "# Display the new column names\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtNdPeR_LahL",
        "outputId": "a577d007-ecb5-429f-86c6-7b45cdde6c46"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['TransactionID', 'TransactionAmt', 'TransactionDT', 'ProductCD',\n",
            "       'User_ID', 'Merchant', 'Card_Number', 'BIN_Number', 'Card_Network',\n",
            "       'Card_Tier', 'Card_Type', 'Phone_Numbers', 'User_Region',\n",
            "       'Order_Region', 'Receiver_Region', 'Distance', 'Sender_email',\n",
            "       'Merchant_email', 'DeviceType', 'DeviceInfo',\n",
            "       'Days_Since_LastTransac_D2', 'SameCard_DaysDiff_D3',\n",
            "       'SameAddress_DaysDiff_D4', 'SameReceiverEmail_DaysDiff_D10',\n",
            "       'SameDeviceType_DaysDiff_D11', 'Device_Matching_M4',\n",
            "       'Device_Mismatch_M6', 'RegionMismatch_M8', 'TransactionConsistency_M9',\n",
            "       'Transaction_Count_C1', 'Unique_Merchants_C4', 'Same_B_region_count_C5',\n",
            "       'Same_Device_count_C6', 'Unique_B_region_C11', 'TransactionTimeSlot_E2',\n",
            "       'HourWithinSlot_E3', 'TransactionWeekday_E4',\n",
            "       'AvgTransactionInterval_E5', 'TransactionAmountVariance_E6',\n",
            "       'TransactionRatio_E7', 'MedianTransactionAmount_E8',\n",
            "       'AvgTransactionAmt_24Hrs_E9', 'TransactionVelocity_E10',\n",
            "       'TimingAnomaly_E11', 'RegionAnomaly_E12', 'HourlyTransactionCount_E13',\n",
            "       'isFraud'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"synthetic_dataset.csv\", index=False)"
      ],
      "metadata": {
        "id": "sf2nvYrXSdGh"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w2tvTp-VWTko"
      },
      "execution_count": 152,
      "outputs": []
    }
  ]
}