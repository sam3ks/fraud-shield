{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "I57NDL8cwRtY"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yMsU_caov3_e"
      },
      "outputs": [],
      "source": [
        "df=pd.read_csv('/content/Synthetic_Dataset.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from scipy import stats\n",
        "import datetime\n",
        "\n",
        "# 1. Remove Duplicate Rows\n",
        "df = df.drop_duplicates()\n",
        "print(f\"Duplicates removed: {df.duplicated().sum()}\")\n",
        "\n",
        "# 2. Handling Missing Values\n",
        "# Numerical: Fill with median, Categorical: Fill with mode\n",
        "# Identify datetime columns first\n",
        "date_cols = [col for col in df.columns if df[col].dtype == 'datetime64[ns]']\n",
        "num_cols = df.select_dtypes(include=[np.number]).columns\n",
        "# Exclude datetime columns from numerical columns\n",
        "num_cols = [col for col in num_cols if col not in date_cols]\n",
        "cat_cols = df.select_dtypes(include=[object]).columns\n",
        "#Check for object columns that are strings, or mixed types.\n",
        "string_like_cols = [col for col in df.columns if (df[col].dtype == 'object') and (df[col].apply(lambda x: isinstance(x,str)).any())]\n",
        "cat_cols = list(set(list(cat_cols) + list(string_like_cols)))\n",
        "\n",
        "num_imputer = SimpleImputer(strategy='median')\n",
        "df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
        "\n",
        "# Check if there are actually any categorical columns before imputing.\n",
        "if len(cat_cols) > 0:\n",
        "    cat_imputer = SimpleImputer(strategy='most_frequent')\n",
        "    df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
        "\n",
        "\n",
        "print(\"Missing values after imputation:\", df.isnull().sum().sum())\n",
        "\n",
        "# 3. Convert Data Types (e.g., DateTime)\n",
        "for col in df.columns:\n",
        "    if 'date' in col.lower() or 'time' in col.lower():\n",
        "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
        "\n",
        "# 4. Handling Inconsistent Data (Trim spaces, fix typos)\n",
        "df = df.apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
        "\n",
        "# 5. Outlier Detection & Handling (IQR method)\n",
        "def remove_outliers_iqr(df, column):\n",
        "    Q1 = df[column].quantile(0.25)\n",
        "    Q3 = df[column].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
        "\n",
        "for col in num_cols:\n",
        "    df = remove_outliers_iqr(df, col)\n",
        "\n",
        "# 6. Encoding Categorical Variables\n",
        "encoder = LabelEncoder()\n",
        "for col in cat_cols:\n",
        "    df[col] = encoder.fit_transform(df[col])\n",
        "\n",
        "# 7. Feature Scaling (Standardization)\n",
        "scaler = StandardScaler()\n",
        "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "# 8. Save Cleaned Data\n",
        "df.to_csv('cleaned_data.csv', index=False)\n",
        "print(\"Data cleaning complete. Cleaned file saved as 'cleaned_data.csv'.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0WKPhDxxXDe",
        "outputId": "b531aebe-9482-4aa8-8727-4e1378690cab"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicates removed: 0\n",
            "Missing values after imputation: 0\n",
            "Data cleaning complete. Cleaned file saved as 'cleaned_data.csv'.\n"
          ]
        }
      ]
    }
  ]
}