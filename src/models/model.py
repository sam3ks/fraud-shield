# -*- coding: utf-8 -*-
"""Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TcRz416NMryd1m2NVJzPdhmByzF5vnkD
"""

!pip install optuna
!pip install xgboost
!pip install pandas
!pip install numpy
!pip install scikit-learn
!pip install joblib

import pandas as pd
import numpy as np
import optuna
import xgboost as xgb
import joblib
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, precision_score, recall_score, f1_score, precision_recall_curve
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import resample
from sklearn.impute import SimpleImputer
import warnings
import logging
import os
from xgboost import XGBClassifier

df=pd.read_csv("/content/synthetic_dataset_272.csv")

def handle_missing_values(df, threshold=0.7):
    # 1. Identify columns with missing values
    missing_percentage = df.isnull().mean()  # Fraction of missing values
    print("Missing values (in %):")
    print(missing_percentage * 100)

    # 2. Drop columns with excessive missing values
    high_missing_cols = missing_percentage[missing_percentage > threshold].index
    print(f"\nColumns dropped (>{threshold*100}% missing): {list(high_missing_cols)}")
    df = df.drop(columns=high_missing_cols)

    # 3. Separate numerical and categorical columns
    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
    categorical_columns = [
                'ProductCD', 'DeviceType', 'Merchant', 'DeviceInfo', 'Card_Network', 'Card_Tier',
                'Card_Type', 'Sender_email', 'Merchant_email', 'User_Region', 'Order_Region',
                'Receiver_Region', 'Device_Matching_M4', 'Phone_Numbers','Device_Mismatch_M6',
                'RegionMismatch_M8','TransactionConsistency_M9', 'TransactionVelocity_E10',
                'TimingAnomaly_E11','RegionAnomaly_E12']

    # 4. Handle missing values for numerical columns (fill with median)
    for col in numerical_cols:
        if df[col].isnull().sum() > 0:
            df[col] = df[col].fillna(df[col].median())

    # 5. Handle missing values for categorical columns (fill with "Missing")
    for col in categorical_columns:
        if df[col].isnull().sum() > 0:
            df[col] = df[col].fillna("Missing")

    print("\nRemaining missing values (should be 0):")
    print(df.isnull().sum())

    return df

# Set the threshold for dropping columns with high missing values
threshold = 0.7
df=handle_missing_values(df, threshold)


# Suppress warnings
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

# Suppress XGBoost warnings
os.environ["XGBOOST_VERBOSITY"] = "0"
logging.getLogger("xgboost").setLevel(logging.WARNING)

# Reduce Optuna logging
optuna.logging.set_verbosity(optuna.logging.WARNING)

df.shape

df.value_counts('isFraud')

def load_and_sample_data(train):
    original_size = len(train)
    print(f"\n🔢 Original Dataset Size: {original_size}")
    print("\n🔍 Original Class Distribution:")
    print(train['isFraud'].value_counts())

    # Separate fraud and non-fraud cases
    fraud = train[train['isFraud'] == 1]
    non_fraud = train[train['isFraud'] == 0]

    # Use all non-fraud cases
    non_fraud_sample_size = len(non_fraud)  # 12,338

    # Calculate required fraud cases to be 20% of the total
    total_target_size = int(non_fraud_sample_size / 0.8)
    fraud_sample_size = total_target_size - non_fraud_sample_size

    # Upsample fraud cases
    fraud_upsampled = resample(fraud, replace=True, n_samples=fraud_sample_size, random_state=53)

    # Combine and shuffle
    sampled_train = pd.concat([fraud_upsampled, non_fraud]).sample(frac=1, random_state=53).reset_index(drop=True)

    # Print sampled dataset statistics
    print(f"\n🔢 Sampled Dataset Size: {len(sampled_train)}")
    print("\n📊 Sampled Class Distribution:")
    print(sampled_train['isFraud'].value_counts())

    return sampled_train

def preprocess_data(train):
    print("\n🧹 Preprocessing data...")

    # Standardize column names
    train.columns = train.columns.str.replace('-', '_')

    # Fill missing values
    train.fillna(-999, inplace=True)

    # Encode categorical features
    label_encoders = {}
    for col in train.select_dtypes(include='object').columns:
        le = LabelEncoder()
        train[col] = le.fit_transform(train[col].astype(str))
        label_encoders[col] = le
    print("✅ Categorical features encoded successfully.")

    # Drop TransactionID
    x = train.drop(['isFraud','TransactionID'], axis=1)
    y = train['isFraud']
    print("✅ Data preprocessing complete.")
    return x,y,label_encoders

def objective(trial, x_train, x_val, y_train, y_val):
    params = {
    'n_estimators': trial.suggest_int('n_estimators', 100, 300),  # Avoid overfitting
    'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.12),
    'max_depth': trial.suggest_int('max_depth', 3, 8),  # Keep trees manageable
    'subsample': trial.suggest_float('subsample', 0.7, 1.0),
    'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 1.0),
    'gamma': trial.suggest_float('gamma', 0, 5),
    'scale_pos_weight':sum(y_train == 0) / sum(y_train == 1),  # Adjusted to 9 (since 90:10 means 9x more legit cases)
    'objective': 'binary:logistic',
    'eval_metric': 'auc',
    'tree_method': 'hist'
    # 'device': 'gpu'
}

    model = XGBClassifier(**params)
    model.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=0)

    val_proba = model.predict_proba(x_val)[:, 1]
    return roc_auc_score(y_val, val_proba)

def train_xgb(x_train, x_val, y_train, y_val):
    print("\n🚀 Running Optuna hyperparameter tuning...")
    study = optuna.create_study(direction='maximize')
    study.optimize(lambda trial: objective(trial, x_train, x_val, y_train, y_val), n_trials=50)

    best_params = study.best_params
    print(f"✅ Best Parameters Found: {best_params}")
    print("\n🔥 Training best XGBoost model on GPU...")
    model = XGBClassifier(**best_params,eval_metric="logloss",early_stopping_rounds=30)
    model.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=100)

    return model

def optimize_threshold(y_val, val_proba):
    print("\n🔧 Optimizing threshold for better F1-score...")
    precision, recall, thresholds = precision_recall_curve(y_val, val_proba)
    f1_scores = 2 * (precision * recall) / (precision + recall)
    best_threshold = thresholds[np.argmax(f1_scores)]
    print(f"🎯 Best Threshold for F1-Score: {best_threshold:.2f}")

    return best_threshold

def evaluate_model(model, x_val, y_val):
    print("\n📊 Evaluating Model...")

    val_proba = model.predict_proba(x_val)[:, 1]
    default_preds = (val_proba > 0.5).astype(int)

    # Compute default scores
    auc_score = roc_auc_score(y_val, val_proba)
    precision = precision_score(y_val, default_preds)
    recall = recall_score(y_val, default_preds)
    f1 = f1_score(y_val, default_preds)

    print("\n📈 Classification Report (Default Threshold):")
    print(classification_report(y_val, default_preds))

    # Optimize threshold
    best_threshold = optimize_threshold(y_val, val_proba)
    optimized_preds = (val_proba >= best_threshold).astype(int)

    # Compute optimized scores
    precision_opt = precision_score(y_val, optimized_preds)
    recall_opt = recall_score(y_val, optimized_preds)
    f1_opt = f1_score(y_val, optimized_preds)

    print("\n📈 Classification Report (Optimized Threshold):")
    print(classification_report(y_val, optimized_preds))

    return auc_score, precision_opt, recall_opt, f1_opt, best_threshold

def main(train):
    # Load and Sample Data
    train_cleaned = load_and_sample_data(train)

    # Preprocess Data
    x, y ,label_encoders= preprocess_data(train_cleaned)
    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.2, random_state=53, stratify=y)

    # Train XGBoost
    model = train_xgb(x_train, x_val, y_train, y_val)

    # Evaluate Model
    auc_score, precision, recall, f1, best_threshold = evaluate_model(model, x_val, y_val)



    print("\n✅ All tasks completed successfully!")

    return model,label_encoders


model,label_encoders= main(df)

# Save the trained model and encoders
joblib.dump(model, 'xgb_fraud_model.pkl')
joblib.dump(label_encoders, 'label_encoders.pkl')







# {
#         'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
#         'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2),
#         'max_depth': trial.suggest_int('max_depth', 3, 12),
#         'subsample': trial.suggest_float('subsample', 0.6, 1.0),
#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
#         'gamma': trial.suggest_float('gamma', 0, 5),
#         'scale_pos_weight': sum(y_train == 0) / sum(y_train == 1),  # Adjusted class weight
#         'objective': 'binary:logistic',
#         'eval_metric': 'auc',
#         'tree_method': 'hist',
#         'device':'gpu'
#     }